"""
LLM æœåŠ¡
è°ƒç”¨ AI Builders Chat Completions API
"""
from typing import List, Dict
import json

import httpx

from config import settings, logger


class LLMService:
    """LLM æœåŠ¡"""
    
    def __init__(self):
        self.api_base = settings.llm.api_base
        self.model = settings.llm.model
        self.api_key = settings.api_key
        self.temperature = settings.llm.temperature
        self.max_tokens = settings.llm.max_tokens
        
        # åˆ›å»º HTTP å®¢æˆ·ç«¯
        self.client = httpx.AsyncClient(
            base_url=self.api_base,
            headers={
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            },
            timeout=httpx.Timeout(
                connect=30.0,
                read=120.0,
                write=60.0,
                pool=10.0
            )
        )
    
    async def generate_answer_stream(
        self, 
        query: str, 
        local_results: List[Dict], 
        web_results: List[Dict]
    ):
        """
        æµå¼ç”Ÿæˆç­”æ¡ˆ - ä½¿ç”¨ SSE æµå¼è¿”å›
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            local_results: æœ¬åœ°æ£€ç´¢ç»“æœ
            web_results: ç½‘ç»œæ£€ç´¢ç»“æœ
        
        Yields:
            str: æµå¼ç”Ÿæˆçš„æ–‡æœ¬ç‰‡æ®µ
        """
        try:
            # æ„å»ºæç¤ºè¯
            prompt = self._build_prompt(query, local_results, web_results)
            
            logger.info(f"ğŸ¤– LLM æµå¼ç”Ÿæˆå¼€å§‹ (model={self.model})")
            
            # è°ƒç”¨ Chat Completions APIï¼ˆæµå¼ï¼‰
            async with self.client.stream(
                "POST",
                "/v1/chat/completions",
                json={
                    "model": self.model,
                    "messages": [
                        {
                            "role": "system",
                            "content": "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½ç¬”è®°åŠ©æ‰‹ï¼Œè´Ÿè´£æ ¹æ®ç”¨æˆ·çš„ç¬”è®°å†…å®¹å’Œç½‘ç»œèµ„æºå›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚è¯·åŸºäºæä¾›çš„æ£€ç´¢ç»“æœç”Ÿæˆå‡†ç¡®ã€æœ‰ç”¨çš„ç­”æ¡ˆã€‚"
                        },
                        {
                            "role": "user",
                            "content": prompt
                        }
                    ],
                    "temperature": self.temperature,
                    "max_tokens": self.max_tokens,
                    "stream": True  # å¯ç”¨æµå¼å“åº”
                }
            ) as response:
                # æ£€æŸ¥çŠ¶æ€ç ï¼ˆå¦‚æœå¤±è´¥ï¼Œæ‰‹åŠ¨è¯»å–å“åº”ä½“å¹¶æŠ›å‡ºå¼‚å¸¸ï¼‰
                if response.status_code >= 400:
                    error_body = await response.aread()
                    logger.error(f"âŒ LLM API è¯·æ±‚å¤±è´¥: {response.status_code}")
                    logger.error(f"âŒ å“åº”ä½“: {error_body.decode('utf-8')}")
                    raise httpx.HTTPStatusError(
                        f"HTTP {response.status_code}",
                        request=response.request,
                        response=response
                    )
                
                logger.info(f"âœ… LLM API è¿æ¥æˆåŠŸï¼Œå¼€å§‹æ¥æ”¶æµå¼æ•°æ®")
                
                # é€è¡Œè¯»å– SSE æµ
                async for line in response.aiter_lines():
                    if not line or line.startswith(":"):
                        continue
                    
                    # ç§»é™¤ "data: " å‰ç¼€
                    if line.startswith("data: "):
                        line = line[6:]
                    
                    # æ£€æŸ¥ç»“æŸæ ‡è®°
                    if line == "[DONE]":
                        logger.info(f"âœ… LLM æµå¼ç”Ÿæˆå®Œæˆ")
                        break
                    
                    try:
                        # è§£æ JSON
                        chunk = json.loads(line)
                        
                        # æå–å†…å®¹
                        if "choices" in chunk and len(chunk["choices"]) > 0:
                            delta = chunk["choices"][0].get("delta", {})
                            content = delta.get("content", "")
                            
                            if content:
                                yield content
                    
                    except json.JSONDecodeError:
                        logger.warning(f"âš ï¸ æ— æ³•è§£æ SSE æ•°æ®: {line}")
                        continue
        
        except httpx.HTTPStatusError as e:
            # è¿™ä¸ªå¼‚å¸¸å·²ç»åœ¨ä¸Šé¢å¤„ç†è¿‡äº†ï¼ˆæ‰‹åŠ¨è¯»å–å“åº”ä½“ï¼‰
            # è¿™é‡Œåªä¼šæ•è·å…¶ä»– HTTP é”™è¯¯
            logger.error(f"âŒ LLM API è¯·æ±‚å¤±è´¥: {e}")
            # é™çº§æ–¹æ¡ˆï¼šè¿”å›å®Œæ•´ç­”æ¡ˆ
            yield self._fallback_answer(query, local_results, web_results)
        
        except Exception as e:
            logger.error(f"âŒ LLM æµå¼ç­”æ¡ˆç”Ÿæˆå¤±è´¥: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            # é™çº§æ–¹æ¡ˆï¼šè¿”å›å®Œæ•´ç­”æ¡ˆ
            yield self._fallback_answer(query, local_results, web_results)
    
    async def generate_answer(
        self, 
        query: str, 
        local_results: List[Dict], 
        web_results: List[Dict]
    ) -> str:
        """
        åŸºäºæ£€ç´¢ç»“æœç”Ÿæˆç­”æ¡ˆï¼ˆéæµå¼ï¼Œä¿ç•™ç”¨äºå…¼å®¹ï¼‰
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            local_results: æœ¬åœ°æ£€ç´¢ç»“æœ
            web_results: ç½‘ç»œæ£€ç´¢ç»“æœ
        
        Returns:
            LLM ç”Ÿæˆçš„ç­”æ¡ˆ
        """
        try:
            # æ„å»ºæç¤ºè¯
            prompt = self._build_prompt(query, local_results, web_results)
            
            logger.info(f"ğŸ¤– LLM è¯·æ±‚å¼€å§‹ (model={self.model}, max_tokens={self.max_tokens})")
            logger.debug(f"ğŸ“ Prompt é•¿åº¦: {len(prompt)} å­—ç¬¦")
            
            # è°ƒç”¨ Chat Completions API
            response = await self.client.post(
                "/v1/chat/completions",
                json={
                    "model": self.model,
                    "messages": [
                        {
                            "role": "system",
                            "content": "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½ç¬”è®°åŠ©æ‰‹ï¼Œè´Ÿè´£æ ¹æ®ç”¨æˆ·çš„ç¬”è®°å†…å®¹å’Œç½‘ç»œèµ„æºå›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚è¯·åŸºäºæä¾›çš„æ£€ç´¢ç»“æœç”Ÿæˆå‡†ç¡®ã€æœ‰ç”¨çš„ç­”æ¡ˆã€‚"
                        },
                        {
                            "role": "user",
                            "content": prompt
                        }
                    ],
                    "temperature": self.temperature,
                    "max_tokens": self.max_tokens
                }
            )
            
            response.raise_for_status()
            data = response.json()
            
            # æå–ç”Ÿæˆçš„ç­”æ¡ˆ
            choice = data["choices"][0]
            answer = choice["message"]["content"]
            finish_reason = choice.get("finish_reason", "unknown")
            
            # è¯¦ç»†æ—¥å¿—è®°å½•
            logger.info(f"âœ… LLM ç­”æ¡ˆç”ŸæˆæˆåŠŸ (model={self.model})")
            logger.info(f"ğŸ“Š å“åº”ç»Ÿè®¡: finish_reason={finish_reason}, ç­”æ¡ˆé•¿åº¦={len(answer)} å­—ç¬¦")
            
            # âš ï¸ é‡è¦ï¼šæ£€æŸ¥æ˜¯å¦å› ä¸º max_tokens é™åˆ¶è€Œæˆªæ–­
            if finish_reason == "length":
                logger.warning(f"âš ï¸ è­¦å‘Šï¼šç­”æ¡ˆå›  max_tokens é™åˆ¶è¢«æˆªæ–­ï¼")
                logger.warning(f"âš ï¸ å½“å‰ max_tokens={self.max_tokens}ï¼Œå»ºè®®å¢åŠ åˆ°è‡³å°‘ {self.max_tokens * 2}")
            
            # è®°å½• token ä½¿ç”¨æƒ…å†µï¼ˆå¦‚æœ API è¿”å›ï¼‰
            if "usage" in data:
                usage = data["usage"]
                logger.info(f"ğŸ”¢ Token ä½¿ç”¨: prompt={usage.get('prompt_tokens', 'N/A')}, "
                          f"completion={usage.get('completion_tokens', 'N/A')}, "
                          f"total={usage.get('total_tokens', 'N/A')}")
            
            return answer
        
        except httpx.HTTPStatusError as e:
            logger.error(f"âŒ LLM API è¯·æ±‚å¤±è´¥: {e.response.status_code}")
            return self._fallback_answer(query, local_results, web_results)
        
        except Exception as e:
            logger.error(f"âŒ LLM ç­”æ¡ˆç”Ÿæˆå¤±è´¥: {str(e)}")
            return self._fallback_answer(query, local_results, web_results)
    
    def _build_prompt(
        self, 
        query: str, 
        local_results: List[Dict], 
        web_results: List[Dict]
    ) -> str:
        """
        æ„å»º LLM æç¤ºè¯
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            local_results: æœ¬åœ°æ£€ç´¢ç»“æœ
            web_results: ç½‘ç»œæ£€ç´¢ç»“æœ
        
        Returns:
            æç¤ºè¯å­—ç¬¦ä¸²
        """
        prompt_parts = [f"ç”¨æˆ·é—®é¢˜ï¼š{query}\n"]
        
        # æ·»åŠ æœ¬åœ°ç¬”è®°å†…å®¹
        if local_results:
            prompt_parts.append("\n## æœ¬åœ°ç¬”è®°ç›¸å…³å†…å®¹ï¼š\n")
            for idx, result in enumerate(local_results[:5], 1):
                title = result.get("title", "æœªçŸ¥æ ‡é¢˜")
                content = result.get("content", "")[:500]  # é™åˆ¶å†…å®¹é•¿åº¦
                prompt_parts.append(f"\n{idx}. ã€{title}ã€‘")
                prompt_parts.append(f"{content}...\n")
        
        # æ·»åŠ ç½‘ç»œèµ„æºå†…å®¹
        if web_results:
            prompt_parts.append("\n## ç½‘ç»œèµ„æºç›¸å…³å†…å®¹ï¼š\n")
            for idx, result in enumerate(web_results[:3], 1):
                title = result.get("title", "æœªçŸ¥æ ‡é¢˜")
                content = result.get("content", "")[:400]  # é™åˆ¶å†…å®¹é•¿åº¦
                prompt_parts.append(f"\n{idx}. ã€{title}ã€‘")
                prompt_parts.append(f"{content}...\n")
        
        # æ·»åŠ æŒ‡å¯¼
        prompt_parts.append("""
\n## å›ç­”è¦æ±‚ï¼š
1. è¯·åŸºäºä¸Šè¿°æ£€ç´¢ç»“æœå›ç­”ç”¨æˆ·çš„é—®é¢˜
2. å¦‚æœæœ¬åœ°ç¬”è®°æœ‰ç›¸å…³å†…å®¹ï¼Œä¼˜å…ˆä½¿ç”¨æœ¬åœ°ç¬”è®°
3. å¦‚æœéœ€è¦è¡¥å……ä¿¡æ¯ï¼Œå¯ä»¥å‚è€ƒç½‘ç»œèµ„æº
4. å›ç­”è¦æ¸…æ™°ã€å‡†ç¡®ã€æœ‰æ¡ç†
5. å¦‚æœæ£€ç´¢ç»“æœæ— æ³•å›ç­”é—®é¢˜ï¼Œè¯·å¦è¯šè¯´æ˜
""")
        
        return ''.join(prompt_parts)
    
    def _fallback_answer(
        self, 
        query: str, 
        local_results: List[Dict], 
        web_results: List[Dict]
    ) -> str:
        """
        é™çº§æ–¹æ¡ˆï¼šLLM å¤±è´¥æ—¶è¿”å›ç®€å•æ ¼å¼åŒ–çš„ç­”æ¡ˆ
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            local_results: æœ¬åœ°æ£€ç´¢ç»“æœ
            web_results: ç½‘ç»œæ£€ç´¢ç»“æœ
        
        Returns:
            æ ¼å¼åŒ–çš„ç­”æ¡ˆ
        """
        logger.warning("âš ï¸ LLM æœåŠ¡ä¸å¯ç”¨ï¼Œä½¿ç”¨é™çº§æ–¹æ¡ˆ")
        
        answer_parts = [f"å…³äºã€Œ{query}ã€ï¼Œæˆ‘æ‰¾åˆ°äº†ä»¥ä¸‹ç›¸å…³å†…å®¹ï¼š\n"]
        
        if local_results:
            answer_parts.append("\nğŸ“š æœ¬åœ°ç¬”è®°ï¼š")
            for idx, result in enumerate(local_results[:5], 1):
                title = result.get("title", "æœªçŸ¥æ ‡é¢˜")
                content = result.get("content", "")[:100]
                answer_parts.append(f"\n{idx}. {title}")
                answer_parts.append(f"   {content}...")
        
        if web_results:
            answer_parts.append("\n\nğŸŒ ç½‘ç»œèµ„æºï¼š")
            for idx, result in enumerate(web_results[:3], 1):
                title = result.get("title", "æœªçŸ¥æ ‡é¢˜")
                content = result.get("content", "")[:100]
                answer_parts.append(f"\n{idx}. {title}")
                answer_parts.append(f"   {content}...")
        
        return '\n'.join(answer_parts)
    
    async def close(self):
        """å…³é—­ HTTP å®¢æˆ·ç«¯"""
        await self.client.aclose()


# æµ‹è¯•ä»£ç 
async def test_llm_service():
    """æµ‹è¯• LLM æœåŠ¡"""
    service = LLMService()
    
    # æ¨¡æ‹Ÿæ£€ç´¢ç»“æœ
    local_results = [
        {
            "title": "Python æ€§èƒ½ä¼˜åŒ–æŠ€å·§",
            "content": "ä½¿ç”¨åˆ—è¡¨æ¨å¯¼å¼æ¯”æ™®é€šå¾ªç¯æ›´å¿«ã€‚ä½¿ç”¨ NumPy å¤„ç†å¤§æ•°ç»„ã€‚é¿å…è¿‡åº¦ä½¿ç”¨å…¨å±€å˜é‡..."
        }
    ]
    
    web_results = [
        {
            "title": "Python Performance Tips",
            "content": "Use built-in functions and libraries. Profile your code. Use appropriate data structures..."
        }
    ]
    
    query = "å¦‚ä½•ä¼˜åŒ– Python ä»£ç æ€§èƒ½ï¼Ÿ"
    answer = await service.generate_answer(query, local_results, web_results)
    
    print(f"é—®é¢˜: {query}")
    print(f"\nç­”æ¡ˆ:\n{answer}")
    
    await service.close()


if __name__ == "__main__":
    import asyncio
    asyncio.run(test_llm_service())
