"""
ç´¢å¼•æœåŠ¡
è´Ÿè´£æ‰«ææ–‡æ¡£ã€æ„å»ºç´¢å¼•
"""
import hashlib
from pathlib import Path
from datetime import datetime
from typing import List, Dict

from tqdm import tqdm

from config import settings, logger
from models import Document, DocumentChunk
from utils import MarkdownParser
from database import MetadataStore, VectorStore
from services.embedder import EmbedderService


class IndexerService:
    """ç´¢å¼•æ„å»ºæœåŠ¡"""
    
    def __init__(self):
        self.notes_dir = Path(settings.notes.directory)
        self.exclude_patterns = settings.notes.exclude_patterns
        self.chunk_size = settings.indexing.chunk_size
        self.chunk_overlap = settings.indexing.chunk_overlap
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.metadata_store = MetadataStore()
        self.vector_store = VectorStore()
        self.embedder = EmbedderService()
        self.parser = MarkdownParser()
        
        self._initialized = False
    
    async def _ensure_initialized(self):
        """ç¡®ä¿æœåŠ¡å·²åˆå§‹åŒ–"""
        if not self._initialized:
            await self.metadata_store.initialize()
            self._initialized = True
    
    def is_index_exists(self) -> bool:
        """æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨"""
        return (
            settings.storage.metadata_db.exists() and
            settings.storage.vector_index.exists()
        )
    
    async def build_index(self):
        """æ„å»ºå®Œæ•´ç´¢å¼•"""
        await self._ensure_initialized()
        
        logger.info(f"ğŸ“š å¼€å§‹æ‰«æç¬”è®°ç›®å½•: {self.notes_dir}")
        
        # 1. æ‰«æ Markdown æ–‡ä»¶
        md_files = self._scan_markdown_files()
        logger.info(f"âœ… æ‰¾åˆ° {len(md_files)} ä¸ª Markdown æ–‡ä»¶")
        
        if not md_files:
            logger.warning("æœªæ‰¾åˆ°ä»»ä½• Markdown æ–‡ä»¶")
            return
        
        # 2. è§£ææ–‡æ¡£
        documents = []
        for file_path in tqdm(md_files, desc="è§£ææ–‡æ¡£"):
            try:
                doc = await self._parse_document(file_path)
                if doc:
                    documents.append(doc)
            except Exception as e:
                logger.error(f"è§£ææ–‡ä»¶å¤±è´¥ {file_path}: {str(e)}")
        
        logger.info(f"âœ… æˆåŠŸè§£æ {len(documents)} ä¸ªæ–‡æ¡£")
        
        # 3. åˆ†å—å¤„ç†
        all_chunks = []
        for doc in tqdm(documents, desc="åˆ†å—å¤„ç†"):
            chunks = self._chunk_document(doc)
            all_chunks.extend(chunks)
        
        logger.info(f"âœ… ç”Ÿæˆ {len(all_chunks)} ä¸ªæ–‡æ¡£å—")
        
        # 4. å‘é‡åŒ–
        logger.info(f"ğŸ”„ å¼€å§‹å‘é‡åŒ– {len(all_chunks)} ä¸ªæ–‡æ¡£å—...")
        texts = [chunk.content for chunk in all_chunks]
        
        try:
            embeddings = await self.embedder.embed_texts(texts, show_progress=True)
            
            # å°†å‘é‡èµ‹å€¼ç»™ chunk
            for chunk, embedding in zip(all_chunks, embeddings):
                chunk.embedding = embedding
            
            logger.info(f"âœ… å‘é‡åŒ–å®Œæˆ")
        except Exception as e:
            logger.error(f"âŒ å‘é‡åŒ–å¤±è´¥: {str(e)}")
            raise
        
        # 5. å­˜å‚¨åˆ°æ•°æ®åº“
        self.vector_store.create_index()
        
        for doc in tqdm(documents, desc="å­˜å‚¨å…ƒæ•°æ®"):
            await self._store_document(doc)
        
        for chunk in tqdm(all_chunks, desc="å­˜å‚¨åˆ†å—"):
            await self._store_chunk(chunk)
        
        # 6. æ„å»ºå‘é‡ç´¢å¼•
        self.vector_store.add_vectors(all_chunks)
        self.vector_store.save()
        
        logger.info(f"âœ… ç´¢å¼•æ„å»ºå®Œæˆï¼")
    
    async def load_index(self):
        """åŠ è½½ç°æœ‰ç´¢å¼•"""
        await self._ensure_initialized()
        
        success = self.vector_store.load()
        if success:
            logger.info("âœ… ç´¢å¼•åŠ è½½æˆåŠŸ")
        else:
            logger.error("âŒ ç´¢å¼•åŠ è½½å¤±è´¥")
    
    def _scan_markdown_files(self) -> List[Path]:
        """æ‰«æ Markdown æ–‡ä»¶"""
        md_files = []
        
        for pattern in ['**/*.md', '**/*.markdown']:
            for file_path in self.notes_dir.glob(pattern):
                # æ£€æŸ¥æ’é™¤æ¨¡å¼
                should_exclude = False
                for exclude in self.exclude_patterns:
                    if exclude.startswith('*.'):
                        # æ–‡ä»¶æ‰©å±•ååŒ¹é…
                        if file_path.suffix == exclude[1:]:
                            should_exclude = True
                            break
                    elif file_path.match(exclude):
                        should_exclude = True
                        break
                
                if not should_exclude:
                    md_files.append(file_path)
        
        return md_files
    
    async def _parse_document(self, file_path: Path) -> Document:
        """è§£æå•ä¸ªæ–‡æ¡£"""
        content, metadata = self.parser.parse_file(file_path)
        
        # æå–åŒé“¾å’Œæ ‡ç­¾
        backlinks = self.parser.extract_backlinks(content)
        tags = self.parser.extract_tags(content)
        
        # æ¸…ç†å†…å®¹
        clean_content = self.parser.clean_content(content)
        
        return Document(
            file_path=file_path,
            content=clean_content,
            title=metadata.get('title', file_path.stem),
            created_at=metadata.get('created_at'),
            modified_at=metadata.get('modified_at'),
            tags=tags,
            backlinks=backlinks,
            metadata=metadata
        )
    
    def _chunk_document(self, doc: Document) -> List[DocumentChunk]:
        """åˆ†å—æ–‡æ¡£"""
        chunks_data = self.parser.chunk_content(
            doc.content,
            chunk_size=self.chunk_size,
            overlap=self.chunk_overlap,
            min_chunk_size=settings.indexing.min_chunk_size
        )
        
        doc_id = self._generate_doc_id(doc.file_path)
        
        chunks = []
        for idx, (chunk_text, start_pos, end_pos) in enumerate(chunks_data):
            chunk_id = f"{doc_id}_chunk_{idx}"
            
            chunk = DocumentChunk(
                chunk_id=chunk_id,
                document_id=doc_id,
                content=chunk_text,
                chunk_index=idx,
                start_pos=start_pos,
                end_pos=end_pos,
                file_path=str(doc.file_path),
                title=doc.title,
                tags=doc.tags,
                backlinks=doc.backlinks,
                created_at=doc.created_at,
                modified_at=doc.modified_at
            )
            
            chunks.append(chunk)
        
        return chunks
    
    async def _store_document(self, doc: Document):
        """å­˜å‚¨æ–‡æ¡£å…ƒæ•°æ®"""
        doc_id = self._generate_doc_id(doc.file_path)
        content_hash = self._hash_content(doc.content)
        
        await self.metadata_store.insert_document(
            doc_id=doc_id,
            file_path=str(doc.file_path),
            title=doc.title,
            created_at=doc.created_at,
            modified_at=doc.modified_at,
            content_hash=content_hash,
            metadata=doc.metadata
        )
        
        # å­˜å‚¨æ ‡ç­¾
        if doc.tags:
            await self.metadata_store.insert_tags(doc_id, doc.tags)
        
        # å­˜å‚¨åŒé“¾
        if doc.backlinks:
            await self.metadata_store.insert_backlinks(doc_id, doc.backlinks)
    
    async def _store_chunk(self, chunk: DocumentChunk):
        """å­˜å‚¨åˆ†å—"""
        await self.metadata_store.insert_chunk(
            chunk_id=chunk.chunk_id,
            doc_id=chunk.document_id,
            content=chunk.content,
            chunk_index=chunk.chunk_index,
            start_pos=chunk.start_pos,
            end_pos=chunk.end_pos
        )
    
    def _generate_doc_id(self, file_path: Path) -> str:
        """ç”Ÿæˆæ–‡æ¡£ID"""
        # ä½¿ç”¨ç›¸å¯¹è·¯å¾„çš„ hash
        rel_path = file_path.relative_to(self.notes_dir)
        return hashlib.md5(str(rel_path).encode()).hexdigest()
    
    def _hash_content(self, content: str) -> str:
        """è®¡ç®—å†…å®¹å“ˆå¸Œ"""
        return hashlib.md5(content.encode()).hexdigest()
    
    async def get_stats(self) -> Dict:
        """è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯"""
        await self._ensure_initialized()
        
        stats = await self.metadata_store.get_stats()
        stats['vector_count'] = self.vector_store.get_size()
        stats['last_update'] = datetime.now().isoformat()
        
        # è®¡ç®—ç´¢å¼•å¤§å°
        index_size = 0
        if settings.storage.vector_index.exists():
            index_size = settings.storage.vector_index.stat().st_size
        stats['index_size_mb'] = round(index_size / 1024 / 1024, 2)
        
        stats['total_files'] = stats['total_documents']
        stats['total_chunks'] = stats['total_chunks']
        
        return stats
    
    async def close(self):
        """å…³é—­æœåŠ¡"""
        await self.metadata_store.close()
        await self.embedder.close()
