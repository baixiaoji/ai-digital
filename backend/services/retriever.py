"""
æ£€ç´¢æœåŠ¡
æ··åˆæ£€ç´¢ï¼šæœ¬åœ°å‘é‡æ£€ç´¢ + ç½‘ç»œæœç´¢
"""
from datetime import datetime, timedelta
from typing import List, Dict
import asyncio

from config import settings, logger
from models import SearchResult
from services.embedder import EmbedderService
from services.web_search import WebSearchService
from services.llm import LLMService


class RetrieverService:
    """æ£€ç´¢æœåŠ¡"""
    
    def __init__(self, indexer):
        self.indexer = indexer
        self.embedder = EmbedderService()
        self.web_search = WebSearchService()
        self.llm = LLMService()
        
        # æ£€ç´¢é…ç½®
        self.time_decay_config = settings.search.time_decay
        self.similarity_threshold = settings.search.similarity_threshold
    
    async def hybrid_search(self, query: str, local_ratio: float = None) -> List[Dict]:
        """
        æ··åˆæ£€ç´¢ï¼ˆæœ¬åœ° + ç½‘ç»œï¼‰
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            local_ratio: æœ¬åœ°ç»“æœå æ¯”ï¼ˆNone åˆ™ä½¿ç”¨é…ç½®ï¼‰
        
        Returns:
            æ’åºåçš„æ£€ç´¢ç»“æœåˆ—è¡¨
        """
        if local_ratio is None:
            local_ratio = settings.search.local_ratio
        
        network_ratio = 1 - local_ratio
        
        # è®¡ç®—æœ¬åœ°å’Œç½‘ç»œçš„ top_k
        total_results = 20
        local_k = int(total_results * local_ratio)
        network_k = int(total_results * network_ratio)
        
        logger.info(f"ğŸ” æ··åˆæ£€ç´¢: local_ratio={local_ratio:.2f}, local_k={local_k}, network_k={network_k}")
        
        # å¹¶å‘æ‰§è¡Œæœ¬åœ°å’Œç½‘ç»œæ£€ç´¢ï¼ˆä¼˜åŒ–ï¼šlocal_k=0 æ—¶è·³è¿‡æœ¬åœ°æ£€ç´¢ï¼‰
        tasks = []
        if local_k > 0:
            tasks.append(self.local_search(query, top_k=local_k))
        else:
            logger.info("â© è·³è¿‡æœ¬åœ°æ£€ç´¢ (local_ratio=0)")
            tasks.append(asyncio.create_task(asyncio.sleep(0)))  # å ä½ä»»åŠ¡
        
        if network_k > 0:
            tasks.append(self.web_search_async(query, top_k=network_k))
        else:
            logger.info("â© è·³è¿‡ç½‘ç»œæ£€ç´¢ (network_ratio=0)")
            tasks.append(asyncio.create_task(asyncio.sleep(0)))  # å ä½ä»»åŠ¡
        
        results = await asyncio.gather(*tasks)
        
        # æå–ç»“æœï¼ˆå¤„ç†å ä½ä»»åŠ¡ï¼‰
        local_results = results[0] if local_k > 0 and isinstance(results[0], list) else []
        web_results = results[1] if network_k > 0 and len(results) > 1 and isinstance(results[1], list) else []
        
        # åˆå¹¶ç»“æœ
        all_results = local_results + web_results
        
        # æŒ‰åˆ†æ•°æ’åº
        all_results.sort(key=lambda x: x.score, reverse=True)
        
        # è½¬æ¢ä¸ºå­—å…¸
        return [result.to_dict() for result in all_results]
    
    async def local_search(self, query: str, top_k: int = 20) -> List[SearchResult]:
        """
        æœ¬åœ°å‘é‡æ£€ç´¢ï¼ˆå¸¦ä¸Šä¸‹æ–‡æ‰©å±•ï¼‰
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›æ•°é‡
        
        Returns:
            æ£€ç´¢ç»“æœåˆ—è¡¨
        """
        # é˜²å¾¡ï¼štop_k <= 0 ç›´æ¥è¿”å›ç©º
        if top_k <= 0:
            logger.info(f"â© æœ¬åœ°æ£€ç´¢è·³è¿‡ (top_k={top_k})")
            return []
        
        logger.info(f"ğŸ” æœ¬åœ°æ£€ç´¢: query=\"{query}\", top_k={top_k}")
        
        # 1. å‘é‡åŒ–æŸ¥è¯¢
        query_vector = await self.embedder.embed_query(query)
        
        # 2. å‘é‡æ£€ç´¢ï¼ˆæ‰©å¤§æœç´¢èŒƒå›´ä»¥ä¾¿åç»­è¿‡æ»¤ï¼‰
        vector_results = self.indexer.vector_store.search(query_vector, top_k=top_k * 3)
        
        if not vector_results:
            logger.warning("æœªæ‰¾åˆ°ç›¸ä¼¼æ–‡æ¡£")
            return []
        
        logger.info(f"ğŸ“Š å‘é‡æ£€ç´¢è¿”å› {len(vector_results)} æ¡å€™é€‰ç»“æœ")
        
        # 3. è·å–åˆ†å—è¯¦ç»†ä¿¡æ¯ï¼ˆå¸¦ä¸Šä¸‹æ–‡æ‰©å±•ï¼‰
        results = []
        for chunk_id, similarity_score in vector_results:
            # ä»å…ƒæ•°æ®åº“è·å–åˆ†å—ä¿¡æ¯ï¼ˆå¸¦ä¸Šä¸‹æ–‡ï¼‰
            chunk_data = await self._get_chunk_data_with_context(
                chunk_id,
                context_before=settings.search.context_before,
                context_after=settings.search.context_after
            )
            
            if chunk_data and similarity_score >= self.similarity_threshold:
                # åº”ç”¨æ—¶é—´è¡°å‡
                time_weight = self._calculate_time_decay(chunk_data['modified_at'])
                
                # ğŸ†• æ ‡é¢˜åŒ¹é…åŠ æƒ
                title_boost = self._calculate_title_boost(query, chunk_data.get('title', ''))
                
                # ç»¼åˆå¾—åˆ†ï¼šå‘é‡ç›¸ä¼¼åº¦ * æ—¶é—´æƒé‡ * æ ‡é¢˜æƒé‡
                final_score = similarity_score * time_weight * title_boost
                
                result = SearchResult(
                    content=chunk_data['extended_content'],  # ä½¿ç”¨æ‰©å±•åçš„å†…å®¹
                    file_path=chunk_data['file_path'],
                    title=chunk_data['title'],
                    score=final_score,
                    source="local",
                    chunk_id=chunk_id,
                    tags=chunk_data.get('tags', []),
                    backlinks=chunk_data.get('backlinks', []),
                    created_at=chunk_data.get('created_at')
                )
                
                results.append(result)
        
        logger.info(f"ğŸ” ç›¸ä¼¼åº¦è¿‡æ»¤: {len(vector_results)} â†’ {len(results)} (é˜ˆå€¼={self.similarity_threshold})")
        
        # æŒ‰åˆ†æ•°æ’åº
        results.sort(key=lambda x: x.score, reverse=True)
        
        # è¿”å› top_k
        results = results[:top_k]
        
        # æ‰“å°è¯¦ç»†æ£€ç´¢ç»“æœï¼ˆæ–¹ä¾¿è°ƒè¯•ï¼‰
        logger.info(f"âœ… æœ¬åœ°æ£€ç´¢å®Œæˆï¼Œè¿”å› {len(results)} æ¡ç»“æœ")
        if results:
            logger.info("ğŸ“‹ æœ¬åœ°å¬å›è¯¦æƒ…:")
            for i, r in enumerate(results, 1):
                content_preview = r.content[:150].replace('\n', ' ')
                logger.info(f"  [{i}] æ–‡ä»¶: {r.file_path}")
                logger.info(f"      åˆ†æ•°: {r.score:.4f} | Chunk: {r.chunk_id}")
                logger.info(f"      å†…å®¹: {content_preview}...")
        
        return results

    
    async def web_search_async(self, query: str, top_k: int = 5) -> List[SearchResult]:
        """
        ç½‘ç»œæœç´¢
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›æ•°é‡
        
        Returns:
            ç½‘ç»œæ£€ç´¢ç»“æœåˆ—è¡¨
        """
        logger.info(f"ğŸŒ ç½‘ç»œæ£€ç´¢: {query}")
        
        web_results = await self.web_search.search(query, max_results=top_k)
        
        results = []
        for item in web_results:
            # ä½¿ç”¨ snippet + content ä½œä¸ºå†…å®¹
            content = item.get('content', '') or item.get('snippet', '')
            
            result = SearchResult(
                content=content,
                file_path="",
                title=item.get('title', ''),
                score=0.5,  # ç½‘ç»œç»“æœå›ºå®šåˆ†æ•°
                source="web",
                url=item.get('url', '')
            )
            
            results.append(result)
        
        logger.info(f"âœ… ç½‘ç»œæ£€ç´¢å®Œæˆï¼Œè¿”å› {len(results)} æ¡ç»“æœ")
        return results
    
    async def _get_chunk_data(self, chunk_id: str) -> Dict:
        """è·å–åˆ†å—æ•°æ®"""
        # ä»å…ƒæ•°æ®åº“æŸ¥è¯¢
        doc_id = chunk_id.rsplit('_chunk_', 1)[0]
        
        cursor = await self.indexer.metadata_store.conn.execute(
            "SELECT * FROM chunks WHERE chunk_id = ?",
            (chunk_id,)
        )
        chunk_row = await cursor.fetchone()
        
        if not chunk_row:
            return None
        
        # è·å–æ–‡æ¡£ä¿¡æ¯
        cursor = await self.indexer.metadata_store.conn.execute(
            "SELECT * FROM documents WHERE doc_id = ?",
            (doc_id,)
        )
        doc_row = await cursor.fetchone()
        
        if not doc_row:
            return None
        
        # è·å–æ ‡ç­¾
        cursor = await self.indexer.metadata_store.conn.execute(
            "SELECT tag_name FROM tags WHERE doc_id = ?",
            (doc_id,)
        )
        tags = [row[0] for row in await cursor.fetchall()]
        
        # è·å–åŒé“¾
        cursor = await self.indexer.metadata_store.conn.execute(
            "SELECT target_page FROM backlinks WHERE source_doc_id = ?",
            (doc_id,)
        )
        backlinks = [row[0] for row in await cursor.fetchall()]
        
        # è§£ææ—¶é—´
        from dateutil import parser as date_parser
        created_at = date_parser.parse(doc_row[3]) if doc_row[3] else None
        modified_at = date_parser.parse(doc_row[4]) if doc_row[4] else None
        
        return {
            "content": chunk_row[2],
            "file_path": doc_row[1],
            "title": doc_row[2],
            "tags": tags,
            "backlinks": backlinks,
            "created_at": created_at,
            "modified_at": modified_at
        }

    
    async def _get_chunk_data_with_context(self, chunk_id: str, 
                                           context_before: int = 3,
                                           context_after: int = 2) -> Dict:
        """
        è·å–åˆ†å—æ•°æ®å¹¶åŒ…å«ä¸Šä¸‹æ–‡
        
        Args:
            chunk_id: chunk ID
            context_before: åŒ…å«å‰é¢ N ä¸ª chunk
            context_after: åŒ…å«åé¢ N ä¸ª chunk
        
        Returns:
            åŒ…å« extended_content çš„ chunk æ•°æ®
        """
        # è·å–å½“å‰ chunk çš„æ•°æ®
        current_chunk = await self._get_chunk_data(chunk_id)
        if not current_chunk:
            return None
        
        # è§£æ doc_id å’Œ chunk_index
        doc_id, chunk_idx_str = chunk_id.rsplit('_chunk_', 1)
        chunk_idx = int(chunk_idx_str)
        
        # è·å–ä¸Šä¸‹æ–‡ chunks
        context_contents = []
        
        # å‰é¢çš„ chunks
        for i in range(chunk_idx - context_before, chunk_idx):
            if i >= 0:
                ctx_id = f"{doc_id}_chunk_{i}"
                cursor = await self.indexer.metadata_store.conn.execute(
                    "SELECT content FROM chunks WHERE chunk_id = ?",
                    (ctx_id,)
                )
                row = await cursor.fetchone()
                if row:
                    context_contents.append(row[0])
        
        # å½“å‰ chunk
        context_contents.append(current_chunk['content'])
        
        # åé¢çš„ chunks
        for i in range(chunk_idx + 1, chunk_idx + context_after + 1):
            ctx_id = f"{doc_id}_chunk_{i}"
            cursor = await self.indexer.metadata_store.conn.execute(
                "SELECT content FROM chunks WHERE chunk_id = ?",
                (ctx_id,)
            )
            row = await cursor.fetchone()
            if row:
                context_contents.append(row[0])
            else:
                break  # æ²¡æœ‰æ›´å¤š chunk äº†
        
        # åˆå¹¶å†…å®¹
        current_chunk['extended_content'] = '\n\n'.join(context_contents)
        
        return current_chunk

    
    def _calculate_title_boost(self, query: str, title: str) -> float:
        """
        è®¡ç®—æ ‡é¢˜åŒ¹é…åŠ æƒ
        
        ç­–ç•¥ï¼š
        1. æå–æŸ¥è¯¢å…³é”®è¯ï¼ˆä¸­æ–‡æŒ‰å­—ç¬¦ï¼Œè‹±æ–‡æŒ‰å•è¯ï¼‰
        2. è®¡ç®—å…³é”®è¯åœ¨æ ‡é¢˜ä¸­çš„è¦†ç›–ç‡
        3. è¿”å›æƒé‡å€æ•°ï¼ˆ1.0 ~ 2.0ï¼‰
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            title: æ–‡æ¡£æ ‡é¢˜
        
        Returns:
            æƒé‡å€æ•°
        """
        if not title or not query:
            return 1.0
        
        # è½¬å°å†™
        query_lower = query.lower()
        title_lower = title.lower()
        
        # åœç”¨è¯
        stopwords = {'çš„', 'äº†', 'å’Œ', 'æ˜¯', 'åœ¨', 'æœ‰', 'æˆ‘', 'ä½ ', 'ä»–', 'å¥¹', 'å®ƒ',
                     'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
                     'å‘Šè¯‰', 'ç¬”è®°', 'ä¸­', 'å“ªäº›', 'ç›¸å…³', 'ä¿¡æ¯', 'å…³äº', 'æœ‰å…³', 'ä¹ˆ', 'å—'}
        
        # æå–å…³é”®è¯ï¼šåˆ†å‰²æˆå•è¯å’Œå­—ç¬¦
        query_keywords = set()
        
        # 1. æŒ‰ç©ºæ ¼/æ ‡ç‚¹åˆ†å‰²ï¼ˆå¤„ç†è‹±æ–‡å•è¯ï¼‰
        import re
        tokens = re.split(r'[\sï¼Œã€‚ï¼ï¼Ÿã€]+', query_lower)
        
        for token in tokens:
            token = token.strip()
            if not token:
                continue
            
            # å¦‚æœæ˜¯è‹±æ–‡å•è¯ï¼ˆé•¿åº¦>=2ï¼‰
            if token.isascii() and len(token) >= 2:
                if token not in stopwords:
                    query_keywords.add(token)
            # å¦‚æœåŒ…å«ä¸­æ–‡ï¼Œæå–2å­—è¯å’Œ3å­—è¯
            else:
                # æå–è¿ç»­çš„éåœç”¨è¯ä¸­æ–‡ç‰‡æ®µ
                for i in range(len(token)):
                    # 2å­—è¯
                    if i + 2 <= len(token):
                        word = token[i:i+2]
                        if word not in stopwords and not word.isascii():
                            query_keywords.add(word)
                    # 3å­—è¯
                    if i + 3 <= len(token):
                        word = token[i:i+3]
                        if word not in stopwords and not word.isascii():
                            query_keywords.add(word)
        
        if not query_keywords:
            return 1.0
        
        # è®¡ç®—åŒ¹é…åº¦
        matched_count = 0
        for keyword in query_keywords:
            if keyword in title_lower:
                matched_count += 1
        
        # è®¡ç®—è¦†ç›–ç‡
        coverage = matched_count / len(query_keywords)
        
        # è¿”å›æƒé‡ï¼š
        # 0% åŒ¹é… â†’ 1.0x (ä¸åŠ æƒ)
        # 50% åŒ¹é… â†’ 1.5x
        # 100% åŒ¹é… â†’ 2.0x (ç¿»å€)
        boost = 1.0 + coverage
        
        if boost > 1.1:  # åªè®°å½•æœ‰æ˜æ˜¾åŠ æƒçš„æƒ…å†µ
            logger.info(f"ğŸ“Œ æ ‡é¢˜åŒ¹é…åŠ æƒ: '{title}' -> {boost:.2f}x (è¦†ç›–ç‡: {coverage:.0%})")
        
        return boost
    
    def _calculate_time_decay(self, modified_at) -> float:
        """
        è®¡ç®—æ—¶é—´è¡°å‡æƒé‡
        
        Args:
            modified_at: ä¿®æ”¹æ—¶é—´
        
        Returns:
            æƒé‡å€æ•°
        """
        if not modified_at:
            return 1.0
        
        now = datetime.now()
        delta = now - modified_at
        
        # è¿‘æœŸï¼ˆ3ä¸ªæœˆå†…ï¼‰ï¼šæƒé‡ Ã— 1.5
        if delta < timedelta(days=self.time_decay_config.recent_months * 30):
            return self.time_decay_config.recent_boost
        
        # æ—§æ–‡æ¡£ï¼ˆ1å¹´å‰ï¼‰ï¼šæƒé‡ Ã— 0.8
        if delta > timedelta(days=self.time_decay_config.old_years * 365):
            return self.time_decay_config.old_penalty
        
        # ä¸­é—´æ—¶æœŸï¼šçº¿æ€§è¡°å‡
        return 1.0
    
    async def format_answer(self, query: str, results: List[SearchResult]):
        """
        æµå¼ç”Ÿæˆç­”æ¡ˆï¼ˆæ‰‹åŠ¨åˆ†å—ï¼‰
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            results: æ£€ç´¢ç»“æœåˆ—è¡¨
        
        Yields:
            str: æ–‡æœ¬ç‰‡æ®µï¼ˆæŒ‰å­—ç¬¦åˆ†å—ï¼‰
        """
        # è½¬æ¢ç»“æœä¸ºå­—å…¸æ ¼å¼
        local_dicts = [r.to_dict() for r in results if r.source == 'local']
        web_dicts = [r.to_dict() for r in results if r.source == 'web']
        
        logger.info(f"ğŸ¨ å¼€å§‹ç”Ÿæˆç­”æ¡ˆ: æœ¬åœ°ç»“æœ={len(local_dicts)}æ¡, ç½‘ç»œç»“æœ={len(web_dicts)}æ¡")
        
        # è°ƒç”¨éæµå¼ LLM API è·å–å®Œæ•´ç­”æ¡ˆ
        try:
            answer = await self.llm.generate_answer(query, local_dicts, web_dicts)
            
            logger.info(f"ğŸ“ ç­”æ¡ˆç”Ÿæˆå®Œæˆï¼Œå¼€å§‹æµå¼åˆ†å—å‘é€ (æ€»é•¿åº¦={len(answer)}å­—ç¬¦)")
            
            # æ‰‹åŠ¨åˆ†å—å‘é€ï¼ˆæ¯æ¬¡ 10 ä¸ªå­—ç¬¦ï¼‰
            chunk_size = 10
            total_chunks = (len(answer) + chunk_size - 1) // chunk_size
            
            for i in range(0, len(answer), chunk_size):
                chunk = answer[i:i + chunk_size]
                chunk_num = i // chunk_size + 1
                logger.debug(f"ğŸ“¤ å‘é€åˆ†å— {chunk_num}/{total_chunks}: {len(chunk)}å­—ç¬¦")
                yield chunk
                # ç¨å¾®å»¶è¿Ÿä»¥æ¨¡æ‹Ÿæµå¼æ•ˆæœ
                await asyncio.sleep(0.05)
            
            logger.info(f"âœ… æµå¼ç­”æ¡ˆå‘é€å®Œæˆ ({total_chunks}ä¸ªåˆ†å—)")
        
        except Exception as e:
            logger.error(f"âŒ ç­”æ¡ˆç”Ÿæˆå¤±è´¥: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            # é™çº§æ–¹æ¡ˆï¼šç®€å•æ ¼å¼åŒ–
            fallback = self.llm._fallback_answer(query, local_dicts, web_dicts)
            logger.warning(f"âš ï¸ ä½¿ç”¨é™çº§æ–¹æ¡ˆç”Ÿæˆç­”æ¡ˆ (é•¿åº¦={len(fallback)}å­—ç¬¦)")
            yield fallback
    
    def format_citations(self, results: List[SearchResult]) -> List[Dict]:
        """
        æ ¼å¼åŒ–å¼•ç”¨ï¼ˆæŒ‰æ–‡ä»¶å»é‡ï¼‰
        
        Args:
            results: æ£€ç´¢ç»“æœ
        
        Returns:
            å¼•ç”¨åˆ—è¡¨ï¼ˆåŒä¸€æ–‡ä»¶åªä¿ç•™å¾—åˆ†æœ€é«˜çš„ï¼‰
        """
        # æŒ‰ file_path åˆ†ç»„ï¼Œä¿ç•™æ¯ä¸ªæ–‡ä»¶å¾—åˆ†æœ€é«˜çš„ç»“æœ
        file_map = {}  # file_path -> (result, max_score)
        
        for result in results:
            # ç½‘ç»œç»“æœä½¿ç”¨ URL ä½œä¸ºå”¯ä¸€æ ‡è¯†
            key = result.url if result.source == "web" else result.file_path
            
            if not key:
                continue
            
            # å¦‚æœæ˜¯æ–°æ–‡ä»¶ï¼Œæˆ–åˆ†æ•°æ›´é«˜ï¼Œåˆ™æ›´æ–°
            if key not in file_map or result.score > file_map[key][1]:
                file_map[key] = (result, result.score)
        
        # æŒ‰åŸå§‹é¡ºåºæ’åºï¼ˆä¿æŒå¾—åˆ†é¡ºåºï¼‰
        unique_results = [item[0] for item in sorted(file_map.values(), key=lambda x: x[1], reverse=True)]
        
        # æ„å»ºå¼•ç”¨
        citations = []
        for idx, result in enumerate(unique_results, 1):
            citation = {
                "id": idx,
                "title": result.title,
                "source": result.source,
            }
            
            if result.source == "local":
                citation["file_path"] = result.file_path
                citation["tags"] = result.tags
                citation["created_at"] = result.created_at.isoformat() if result.created_at else None
            else:
                citation["url"] = result.url
            
            citations.append(citation)
        
        logger.info(f"ğŸ“š å¼•ç”¨å»é‡: {len(results)} æ¡ç»“æœ â†’ {len(citations)} ä¸ªå”¯ä¸€æ–‡ä»¶/URL")
        return citations
