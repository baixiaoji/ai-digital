"""
Markdown è§£æå™¨
æ”¯æŒ Logseq åŒé“¾è¯­æ³•å’Œæ ‡ç­¾æå–
"""
import re
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Tuple

import frontmatter


class MarkdownParser:
    """Markdown æ–‡æ¡£è§£æå™¨"""
    
    # æ­£åˆ™è¡¨è¾¾å¼
    BACKLINK_PATTERN = re.compile(r'\[\[([^\]]+)\]\]')  # [[é¡µé¢å]]
    TAG_PATTERN = re.compile(r'(?:^|\s)#([a-zA-Z0-9_\u4e00-\u9fa5]+)')  # #æ ‡ç­¾
    
    @staticmethod
    def parse_file(file_path: Path) -> Tuple[str, Dict]:
        """
        è§£æ Markdown æ–‡ä»¶
        
        Returns:
            (content, metadata) å…ƒç»„
        """
        with open(file_path, 'r', encoding='utf-8') as f:
            post = frontmatter.load(f)
        
        # æå–å…ƒæ•°æ®
        metadata = dict(post.metadata)
        content = post.content
        
        # æå–æ–‡ä»¶æ—¶é—´
        stat = file_path.stat()
        metadata['created_at'] = datetime.fromtimestamp(stat.st_ctime)
        metadata['modified_at'] = datetime.fromtimestamp(stat.st_mtime)
        
        # æå–æ ‡é¢˜ï¼ˆä¼˜å…ˆçº§ï¼šfrontmatter > æ–‡ä»¶åï¼‰
        if 'title' not in metadata:
            metadata['title'] = file_path.stem
        
        return content, metadata
    
    @staticmethod
    def extract_backlinks(content: str) -> List[str]:
        """
        æå–åŒé“¾å¼•ç”¨ [[é¡µé¢å]]
        
        Args:
            content: æ–‡æ¡£å†…å®¹
        
        Returns:
            åŒé“¾åˆ—è¡¨
        """
        matches = MarkdownParser.BACKLINK_PATTERN.findall(content)
        return list(set(matches))  # å»é‡
    
    @staticmethod
    def extract_tags(content: str) -> List[str]:
        """
        æå–æ ‡ç­¾ #tag
        
        Args:
            content: æ–‡æ¡£å†…å®¹
        
        Returns:
            æ ‡ç­¾åˆ—è¡¨
        """
        matches = MarkdownParser.TAG_PATTERN.findall(content)
        return list(set(matches))  # å»é‡
    
    @staticmethod
    def clean_content(content: str) -> str:
        """
        æ¸…ç†å†…å®¹ï¼ˆç§»é™¤ Markdown è¯­æ³•ï¼‰
        
        Args:
            content: åŸå§‹å†…å®¹
        
        Returns:
            æ¸…ç†åçš„çº¯æ–‡æœ¬
        """
        # ç§»é™¤ä»£ç å—
        content = re.sub(r'```[\s\S]*?```', '', content)
        
        # ç§»é™¤è¡Œå†…ä»£ç 
        content = re.sub(r'`[^`]+`', '', content)
        
        # ç§»é™¤å›¾ç‰‡
        content = re.sub(r'!\[.*?\]\(.*?\)', '', content)
        
        # ç§»é™¤é“¾æ¥ï¼ˆä¿ç•™æ–‡æœ¬ï¼‰
        content = re.sub(r'\[([^\]]+)\]\([^\)]+\)', r'\1', content)
        
        # ç§»é™¤åŒé“¾æ ‡è®°ï¼ˆä¿ç•™æ–‡æœ¬ï¼‰
        content = re.sub(r'\[\[([^\]]+)\]\]', r'\1', content)
        
        # ç§»é™¤æ ‡é¢˜æ ‡è®°
        content = re.sub(r'^#{1,6}\s+', '', content, flags=re.MULTILINE)
        
        # ç§»é™¤åŠ ç²—/æ–œä½“
        content = re.sub(r'\*\*([^\*]+)\*\*', r'\1', content)
        content = re.sub(r'\*([^\*]+)\*', r'\1', content)
        
        # ç§»é™¤å¼•ç”¨æ ‡è®°
        content = re.sub(r'^>\s+', '', content, flags=re.MULTILINE)
        
        # ç§»é™¤åˆ—è¡¨æ ‡è®°
        content = re.sub(r'^\s*[-*+]\s+', '', content, flags=re.MULTILINE)
        content = re.sub(r'^\s*\d+\.\s+', '', content, flags=re.MULTILINE)
        
        # ç§»é™¤å¤šä½™ç©ºè¡Œ
        content = re.sub(r'\n{3,}', '\n\n', content)
        
        return content.strip()
    
    @staticmethod
    def chunk_content(content: str, chunk_size: int = 300, overlap: int = 80, 
                      min_chunk_size: int = 100) -> List[Tuple[str, int, int]]:
        """
        æ··åˆç­–ç•¥åˆ†å—ï¼šä¼˜å…ˆæ®µè½ + æ™ºèƒ½å¥å­è¾¹ç•Œ + å°æ–‡ä»¶ä¿æŠ¤
        
        ç­–ç•¥ï¼š
        1. ã€æ–°å¢ã€‘è‹¥æ–‡æ¡£é•¿åº¦ < chunk_sizeï¼Œæ•´ä½“ä½œä¸º1ä¸ªchunkï¼ˆä¸åˆ†å‰²ï¼‰
        2. ä¼˜å…ˆæŒ‰æ®µè½ï¼ˆåŒæ¢è¡Œï¼‰åˆ†å‰²
        3. æ®µè½è¿‡å¤§æ—¶ï¼ŒæŒ‰å¥å­è¾¹ç•Œç»†åˆ†
        4. ç¡®ä¿ chunk å¤§å°åœ¨ [min_chunk_size, chunk_size] èŒƒå›´
        5. è¿‡æ»¤è¿‡çŸ­çš„ chunk
        
        Args:
            content: æ–‡æ¡£å†…å®¹
            chunk_size: ç›®æ ‡å—å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
            overlap: é‡å å­—ç¬¦æ•°
            min_chunk_size: æœ€å°åˆ†å—å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
        
        Returns:
            [(chunk_text, start_pos, end_pos), ...]
        """
        chunks = []
        
        # ğŸ†• å°æ–‡ä»¶ä¿æŠ¤ï¼šè‹¥å†…å®¹ < chunk_sizeï¼Œæ•´ä½“ä½œä¸º1ä¸ªchunk
        if len(content) < chunk_size:
            if content.strip():  # ç¡®ä¿ä¸æ˜¯ç©ºå†…å®¹
                return [(content, 0, len(content))]
            else:
                return []  # ç©ºå†…å®¹è¿”å›ç©ºåˆ—è¡¨
        
        # ç¬¬ä¸€æ­¥ï¼šæŒ‰æ®µè½åˆ†å‰²
        paragraphs = content.split('\n\n')
        
        current_pos = 0
        accumulated_text = ""
        accumulated_start = 0
        
        for paragraph in paragraphs:
            paragraph = paragraph.strip()
            if not paragraph:
                current_pos += 2  # è·³è¿‡ \n\n
                continue
            
            # å¦‚æœç´¯ç§¯æ–‡æœ¬ä¸ºç©ºï¼Œå¼€å§‹æ–°çš„ç´¯ç§¯
            if not accumulated_text:
                accumulated_text = paragraph
                accumulated_start = current_pos
            else:
                # å°è¯•åŠ å…¥å½“å‰æ®µè½
                test_text = accumulated_text + '\n\n' + paragraph
                
                # å¦‚æœåŠ å…¥åè¶…è¿‡ç›®æ ‡å¤§å°ï¼Œå¤„ç†ç´¯ç§¯çš„æ–‡æœ¬
                if len(test_text) > chunk_size:
                    # å¤„ç†ä¹‹å‰ç´¯ç§¯çš„æ–‡æœ¬
                    if len(accumulated_text) >= min_chunk_size:
                        # å¦‚æœç´¯ç§¯æ–‡æœ¬è¿‡å¤§ï¼Œéœ€è¦ç»†åˆ†
                        if len(accumulated_text) > chunk_size * 1.5:
                            sub_chunks = MarkdownParser._split_large_text(
                                accumulated_text, accumulated_start, 
                                chunk_size, overlap, min_chunk_size
                            )
                            chunks.extend(sub_chunks)
                        else:
                            chunks.append((accumulated_text, accumulated_start, 
                                         accumulated_start + len(accumulated_text)))
                    
                    # å¼€å§‹æ–°çš„ç´¯ç§¯
                    accumulated_text = paragraph
                    accumulated_start = current_pos
                else:
                    # ç»§ç»­ç´¯ç§¯
                    accumulated_text = test_text
            
            current_pos += len(paragraph) + 2  # åŒ…æ‹¬ \n\n
        
        # å¤„ç†æœ€åçš„ç´¯ç§¯æ–‡æœ¬
        if accumulated_text and len(accumulated_text) >= min_chunk_size:
            if len(accumulated_text) > chunk_size * 1.5:
                sub_chunks = MarkdownParser._split_large_text(
                    accumulated_text, accumulated_start, 
                    chunk_size, overlap, min_chunk_size
                )
                chunks.extend(sub_chunks)
            else:
                chunks.append((accumulated_text, accumulated_start, 
                             accumulated_start + len(accumulated_text)))
        
        return chunks
    
    @staticmethod
    def _split_large_text(text: str, start_offset: int, chunk_size: int, 
                         overlap: int, min_chunk_size: int) -> List[Tuple[str, int, int]]:
        """
        åˆ†å‰²è¿‡å¤§çš„æ–‡æœ¬ï¼ˆæŒ‰å¥å­è¾¹ç•Œï¼‰
        
        Args:
            text: è¦åˆ†å‰²çš„æ–‡æœ¬
            start_offset: æ–‡æœ¬åœ¨åŸæ–‡æ¡£ä¸­çš„èµ·å§‹ä½ç½®
            chunk_size: ç›®æ ‡å—å¤§å°
            overlap: é‡å å¤§å°
            min_chunk_size: æœ€å°å—å¤§å°
        
        Returns:
            [(chunk_text, start_pos, end_pos), ...]
        """
        chunks = []
        start = 0
        text_len = len(text)
        
        while start < text_len:
            # ç†æƒ³çš„ç»“æŸä½ç½®
            ideal_end = min(start + chunk_size, text_len)
            
            # å¦‚æœå·²åˆ°è¾¾æœ«å°¾
            if ideal_end >= text_len:
                end = text_len
            else:
                # åœ¨åˆç†èŒƒå›´å†…æŸ¥æ‰¾å¥å­è¾¹ç•Œ
                # æœç´¢èŒƒå›´ï¼š[min_size, ideal_end]
                search_start = max(start + min_chunk_size, ideal_end - 200)
                search_end = ideal_end
                
                # æŸ¥æ‰¾æœ€ä½³åˆ†éš”ç¬¦ä½ç½®
                best_pos = -1
                # ä¼˜å…ˆçº§ï¼šä¸­æ–‡å¥å· > ä¸­æ–‡æ ‡ç‚¹ > åŒæ¢è¡Œ > è‹±æ–‡å¥å·
                for delimiter in ['ã€‚', 'ï¼', 'ï¼Ÿ', '\n\n', '.', '!', '?']:
                    pos = text.rfind(delimiter, search_start, search_end)
                    if pos > best_pos:
                        best_pos = pos
                
                # å¦‚æœæ‰¾åˆ°åˆé€‚çš„åˆ†éš”ç¬¦
                if best_pos != -1:
                    end = best_pos + 1
                else:
                    # å¦åˆ™åœ¨ç©ºæ ¼å¤„åˆ†å‰²
                    space_pos = text.rfind(' ', search_start, search_end)
                    if space_pos != -1:
                        end = space_pos + 1
                    else:
                        # å®åœ¨æ‰¾ä¸åˆ°ï¼Œå¼ºåˆ¶åˆ†å‰²
                        end = ideal_end
            
            # æå– chunk
            chunk_text = text[start:end].strip()
            
            # è¿‡æ»¤è¿‡çŸ­çš„ chunk
            if len(chunk_text) >= min_chunk_size:
                chunks.append((
                    chunk_text,
                    start_offset + start,
                    start_offset + end
                ))
            
            # è®¡ç®—ä¸‹ä¸€ä¸ªèµ·å§‹ä½ç½®ï¼ˆå¸¦é‡å ï¼‰
            start = end - overlap
            
            # é˜²æ­¢æ­»å¾ªç¯ï¼šç¡®ä¿è‡³å°‘å‰è¿›
            if start <= (chunks[-1][1] - start_offset if chunks else -1):
                start = end
            
            # å¦‚æœä¸‹ä¸€æ¬¡è¿­ä»£ä¸ä¼šäº§ç”Ÿè¶³å¤Ÿå¤§çš„ chunkï¼Œç›´æ¥é€€å‡º
            if text_len - start < min_chunk_size:
                break
        
        return chunks


class LogseqParser(MarkdownParser):
    """Logseq ä¸“ç”¨è§£æå™¨ï¼ˆæ‰©å±•åŠŸèƒ½ï¼‰"""
    
    @staticmethod
    def parse_properties(content: str) -> Dict:
        """
        è§£æ Logseq properties
        
        Example:
            - property:: value
            - tags:: #tag1 #tag2
        """
        properties = {}
        pattern = re.compile(r'^\s*-\s*(\w+)::\s*(.+)$', re.MULTILINE)
        
        for match in pattern.finditer(content):
            key = match.group(1)
            value = match.group(2).strip()
            properties[key] = value
        
        return properties
